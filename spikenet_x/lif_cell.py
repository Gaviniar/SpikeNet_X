# -*- coding: utf-8 -*-
"""
LIFCell: è„‰å†²ç¥ç»å…ƒå•å…ƒï¼ˆæ”¯æŒè‡ªé€‚åº”é˜ˆå€¼ä¸ fast-tanh ä»£ç†æ¢¯åº¦ï¼‰

æ¥å£
----
forward(M: Float[T, N, d]) -> Tuple[S: Float[T, N], V: Float[T, N], aux: Dict]
- M ä¸ºä»èšåˆå™¨å¾—åˆ°çš„æ¶ˆæ¯ï¼ˆç”µæµè¾“å…¥ï¼‰
- å…ˆç”¨çº¿æ€§æŠ•å½± U: R^d -> R å°†é€šé“èšåˆä¸ºæ ‡é‡ç”µæµ I_tn
- é€’æ¨æ›´æ–°è†œç”µä½ä¸é˜ˆå€¼ï¼Œäº§ç”Ÿè„‰å†²

å‚è€ƒå…¬å¼ï¼ˆæç¤ºè¯ï¼‰
----------------
V_{i,t} = Î» V_{i,t-1} + U m_{i,t} - Î¸_{i,t-1} R_{i,t-1}
S_{i,t} = ğŸ™[V_{i,t} > Î¸_{i,t}]
V_{i,t} â† V_{i,t} - S_{i,t} Â· Î¸_{i,t}          (é‡ç½®)
Î¸_{i,t} = Ï„_Î¸ Î¸_{i,t-1} + Î³ S_{i,t-1}          (è‡ªé€‚åº”é˜ˆå€¼ï¼Œå¯é€‰)

è®­ç»ƒ
----
- ä½¿ç”¨ fast-tanh ä»£ç†æ¢¯åº¦:
  y = H(x) + (tanh(Î²x) - tanh(Î²x).detach())
  å…¶ä¸­ H(x) ä¸ºç¡¬é˜¶è·ƒ (x>0)
"""

from __future__ import annotations

from typing import Dict, Optional, Tuple

import torch
import torch.nn as nn


def _fast_tanh_surrogate(x: torch.Tensor, beta: float = 2.0) -> torch.Tensor:
    """
    ç¡¬è§¦å‘ + å¹³æ»‘æ¢¯åº¦çš„ STE å®ç°:
      forward: step(x)
      backward: tanh(Î²x) çš„å¯¼æ•° (â‰ˆ Î² * (1 - tanh^2(Î²x)))
    """
    hard = (x > 0).to(x.dtype)
    soft = torch.tanh(beta * x)
    return hard + (soft - soft.detach())


class LIFCell(nn.Module):
    def __init__(
        self,
        d: int,
        lambda_mem: float = 0.95,
        tau_theta: float = 0.99,
        gamma: float = 0.10,
        adaptive: bool = True,
        surrogate: str = "fast_tanh",
        beta: float = 2.0,
    ) -> None:
        super().__init__()
        assert 0.0 <= lambda_mem <= 1.0
        assert 0.0 <= tau_theta <= 1.0
        assert gamma >= 0.0

        self.d = int(d)
        self.adaptive = bool(adaptive)
        self.surrogate = str(surrogate)
        self.beta = float(beta)

        # U: R^d -> Rï¼ˆå…±äº«äºæ‰€æœ‰èŠ‚ç‚¹ï¼‰ï¼Œæ— åç½®é¿å…ç”µæµæ¼‚ç§»
        self.proj = nn.Linear(d, 1, bias=False)

        # å°†æ ‡é‡å‚æ•°æ³¨å†Œä¸º bufferï¼Œä¾¿äºè„šæœ¬åŒ–ä¸ç§»åŠ¨è®¾å¤‡
        self.register_buffer("lambda_mem", torch.as_tensor(lambda_mem, dtype=torch.float32))
        self.register_buffer("tau_theta", torch.as_tensor(tau_theta, dtype=torch.float32))
        self.register_buffer("gamma", torch.as_tensor(gamma, dtype=torch.float32))

    def _spike(self, x: torch.Tensor) -> torch.Tensor:
        if self.surrogate == "fast_tanh":
            return _fast_tanh_surrogate(x, beta=self.beta)
        # å…œåº•ï¼šçº¯ç¡¬é˜ˆå€¼ï¼ˆæ— ä»£ç†æ¢¯åº¦ï¼‰
        return (x > 0).to(x.dtype)

    @torch.no_grad()
    def reset_parameters(self) -> None:
        nn.init.xavier_uniform_(self.proj.weight)

    def forward(
        self,
        M: torch.Tensor,                # [T, N, d]
        state0: Optional[Dict] = None,  # å¯é€‰: {"V": [N], "theta": [N], "S": [N]}
    ) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
        assert M.dim() == 3, "M å½¢çŠ¶åº”ä¸º [T, N, d]"
        T, N, d = M.shape
        assert d == self.d, f"d ä¸åŒ¹é…: æœŸæœ› {self.d}, å®å¾— {d}"

        device = M.device
        dtype = M.dtype

        # åˆå§‹çŠ¶æ€
        if state0 is None:
            V = torch.zeros(N, device=device, dtype=dtype)
            theta = torch.ones(N, device=device, dtype=dtype)  # åˆå§‹é˜ˆå€¼ 1.0
            S_prev = torch.zeros(N, device=device, dtype=dtype)
        else:
            V = state0.get("V", torch.zeros(N, device=device, dtype=dtype)).to(dtype)
            theta = state0.get("theta", torch.ones(N, device=device, dtype=dtype)).to(dtype)
            S_prev = state0.get("S", torch.zeros(N, device=device, dtype=dtype)).to(dtype)

        S_seq = []
        V_seq = []
        theta_seq = []

        lam = self.lambda_mem
        tau = self.tau_theta
        gam = self.gamma

        for t in range(T):
            # æŠ•å½±åˆ°æ ‡é‡ç”µæµ I_tn: [N]
            I = self.proj(M[t]).squeeze(-1)  # [N]

            # è®°å¿†è¡°å‡ + è¾“å…¥ç´¯ç§¯
            V = lam * V + I - (theta * S_prev)  # åŒ…å«ä¸Šä¸€æ­¥çš„ Refractory æŠ‘åˆ¶é¡¹

            # è§¦å‘æ¡ä»¶ä¸ä»£ç†æ¢¯åº¦
            x = V - theta
            S = self._spike(x)  # [N] in [0,1]

            # é‡ç½®ï¼šå‘æ”¾å¤„æ‰£é™¤é˜ˆå€¼
            V = V - S * theta

            # è‡ªé€‚åº”é˜ˆå€¼
            if self.adaptive:
                theta = tau * theta + gam * S_prev

            # è®°å½•
            S_seq.append(S)
            V_seq.append(V)
            theta_seq.append(theta)

            # æ›´æ–°ä¸Šä¸€æ—¶åˆ»çš„å‘æ”¾
            S_prev = S

        S_out = torch.stack(S_seq, dim=0)  # [T, N]
        V_out = torch.stack(V_seq, dim=0)  # [T, N]

        aux = {
            "theta": torch.stack(theta_seq, dim=0),   # [T, N]
            "spike_rate": S_out.mean().detach(),      # æ ‡é‡ï¼Œä¾¿äºç›‘æ§
        }
        return S_out, V_out, aux
