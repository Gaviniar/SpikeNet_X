# SpikeNet-X 技术规格（实现提示词）

## 0. 目标与总览
**目标**：在动态图/时序图上，用事件驱动的**脉冲时序注意力聚合（STA）**替换传统时间池化，并在其前端串联**可学习多延迟通路（DelayLine）**，显式建模关系传播的**时间错位与传播时延**，保持 SNN 的稀疏事件驱动与可解释性。

**层级结构（自上而下）**  
1) **DelayLine**：沿时间维的**因果深度可分离 1D 卷积**（K 个离散延迟 tap），学习不同延迟的响应；  
2) **STA-GNN Aggregator**：对邻居在过去时间的事件序列进行**因果多头注意力**，以**相对时间编码**与**脉冲门控**（surrogate）做软选择，并用 **Top-k 稀疏化**控制成本；  
3) **脉冲单元（LIF/GLIF）**：将上一步的聚合消息注入膜电位，阈值比较产生脉冲，使用代理梯度反传。

---

## 1. 记号与输入输出约定

- 时长 \(T\)、节点数 \(N\)、特征维 \(d_\text{in}\)、隐藏维 \(d\)、注意力头数 \(H\)、延迟 tap 数 \(K\)、相对时间编码维 \(d_\text{pe}\)。  
- 默认采用**时间优先格式**：`H_tn` 表示 `[T, N, d]`；脉冲张量 `S` 为 `[T, N]`（0/1 或 {0,1} 的浮点）。  
- 图结构用 `edge_index: LongTensor[2, E]`（PyG 风格），或邻接稀疏矩阵。若使用稠密掩码 `adj_mask: Bool[N, N]` 亦可。  
- **Batch**：建议将多图拼成大图，用 `batch: LongTensor[N_total]` 指示归属；若必须 `B×N×T×d`，可在外层再套一维 batch 并展开为大图。  
- **模块 API（核心）**：
  - `LearnableDelayLine.forward(H: [T, N, d_in]) -> H̃: [T, N, d_in]`
  - `SpikingTemporalAttention.forward(H̃: [T,N,d_qkv], S: [T,N], edge_index or adj_mask, time_idx: [T]) -> M: [T,N,d]`
  - `LIFCell.forward(M: [T,N,d], state0) -> (S: [T,N], V: [T,N], aux)`  
  其中 `d_qkv` 与 `d` 可相同或通过线性层投影。

---

## 2. DelayLine（可学习多延迟通路）

### 原理
对每个节点、每个通道沿时间做**因果卷积**，学习对不同**传播时延**的权重偏好。  
\[
\tilde{h}_{t}^{(c)}=\sum_{k=0}^{K-1} w_k^{(c)} \cdot h_{t-k}^{(c)},\quad 
w_k^{(c)}=\frac{\text{softplus}(u_k^{(c)})}{\sum_r \text{softplus}(u_r^{(c)})}\cdot \rho^k,\ \rho\in(0,1)
\]
- `softplus`+**归一化**保证稳定、可解释（权重非负，且随延迟指数折扣 \(\rho^k\)）。  
- 支持**通道共享**（每层统一一组 \(\{w_k\}\)）或**逐通道**（推荐：逐通道但分组实现）。

### 接口与形状
- 入：`H: Float[T, N, d_in]`  
- 出：`H_tilde: Float[T, N, d_in]`（与输入同形）  
- 可选参数：`K=3~7`，`rho=0.85`，`per_channel=True`，`causal_pad='left'`。

### 计算与实现要点
- **实现**：等价 `groups=d_in` 的 `Conv1d`（深度可分离），输入先转置为 `[N*d_in, 1, T]` 做分组 1D 卷积后还原；或用自定义 `causal_depthwise_conv1d`。  
- **复杂度**：\(O(T\cdot N\cdot d_\text{in}\cdot K)\)。  
- **正则**（可选）：`L1(w)` 或 `Entropy(w)` 促进稀疏/清晰峰值。  
- **边界**：对 `t<0` 采用零填充。

---

## 3. STA-GNN（脉冲时序注意力聚合）

### 核心思想
在**因果掩码**与**邻接掩码**下，使用**相对时间编码**的多头注意力，让每个节点在时刻 \(t\) 选择来自邻居在**过去各时刻 \(t' \le t\)** 的关键事件，并用**脉冲发放**对注意力进行**门控**，保持事件驱动与可导性。

### 相对时间编码 \(\phi(\Delta t)\)
- 组合基函数（推荐维度 \(d_\text{pe}=8\)）：  
  - 指数衰减：\(\exp(-\Delta t/\tau_m)\), \(\exp(-\Delta t/\tau_s)\)  
  - 正弦基：\(\sin(\omega_r \Delta t),\ \cos(\omega_r \Delta t)\)（对数均匀频率）  
  - 分桶 one-hot：\(\text{bucket}(\Delta t)\)（对数间隔）  
- 预计算 `PE: Float[T, T, d_pe]`，仅在 \(t' \le t\) 使用。

### 注意力计算（单头，后接 H 头拼接）
给定 `H̃`（DelayLine 之后）：
\[
\begin{aligned}
q_{i,t}&=W_Q h_{i,t},\\
k_{j,t'}&=W_K [h_{j,t'} \,\|\, \phi(t-t')],\\
v_{j,t'}&=W_V h_{j,t'}.
\end{aligned}
\]
**门控**：将源端脉冲 \(s_{j,t'}\in\{0,1\}\) 通过 surrogate \(\sigma_{\text{sur}}(\cdot)\) 映射到 \([0,1]\)，作为注意力值的可导缩放因子。  
\[
e_{i,t,j,t'}=\frac{q_{i,t}\cdot k_{j,t'}}{\sqrt{d/H}} + b_{\Delta t} \quad (\text{可选相对偏置})
\]
\[
a_{i,t,j,t'}=\text{softmax}_{(j,t')\in\mathcal{N}(i),t'\le t}\big(e_{i,t,j,t'}\big)\cdot \sigma_{\text{sur}}(s_{j,t'})
\]
\[
m_{i,t}=\sum_{t'\le t}\sum_{j\in\mathcal{N}(i)} a_{i,t,j,t'} \, v_{j,t'}
\]

### 稀疏化与掩码
- **因果掩码**：仅允许 \(t'\le t\)。  
- **邻接掩码**：仅允许 `j ∈ N(i)`。  
- **时间窗**：限制 \(t-t'\le W\)（建议 `W∈[16,128]` 根据任务/显存选择）。  
- **Top-k**：对每个 `(i,t)` 仅保留前 `k` 个 key（在 `(j,t')` 维度上），其余置零并重新归一化（`k=8~32`）。

### 形状与接口
- 入：  
  - `H_tilde: Float[T, N, d_in]`  
  - `S: Float[T, N]`（0/1）  
  - `edge_index: Long[2, E]` 或 `adj_mask: Bool[N,N]`  
  - `time_idx: Long[T]`（通常为 `torch.arange(T)`）  
- 出：`M: Float[T, N, d]`  
- 可选：`dropout_attn`, `attn_temperature`, `relative_bias=True/False`。

### 复杂度与两种实现模式
- **Dense 模式（小图/短序列）**：构建 `[T,T,N,N]` 掩码后矩阵化，配合 Top-k；实现简单，峰值显存较高。  
- **Sparse-edge 模式（推荐）**：以 `edge_index` 为骨架，仅对边上的 `(i,j)` 计算注意力；按时间窗为每条边滚动收集 \(t'\in[t-W,t]\) 的 key，使用分块与 `segment_softmax`。复杂度近似 \(O(H\cdot E\cdot W)\)。

---

## 4. 脉冲神经元（LIF/GLIF）与集成

### LIF 更新（可换成你已有实现）
\[
\begin{aligned}
V_{i,t} &= \lambda V_{i,t-1}+ U m_{i,t} - \theta_{i,t-1} R_{i,t-1} \\
S_{i,t} &= \mathbb{1}[V_{i,t} > \theta_{i,t}] \\
V_{i,t} &\leftarrow V_{i,t} - S_{i,t}\cdot \theta_{i,t} \quad (\text{重置}) \\
\theta_{i,t} &= \tau_\theta \theta_{i,t-1} + \gamma S_{i,t-1} \quad (\text{自适应阈值，可选})
\end{aligned}
\]
- **反传**：使用 surrogate \(\sigma_{\text{sur}}'(V-\theta)\)（fast-tanh 或 piecewise-linear STE）。  
- **接口**：`LIFCell(M: [T,N,d]) -> S: [T,N], V: [T,N], (theta/R 等可选)`

---

## 5. SpikeNet-X 层与前向流程

### SpikeNet-X 层伪接口
```python
class SpikeNetXLayer(nn.Module):
    def __init__(self, d_in, d, heads=4, topk=16, W=64, K=5, rho=0.85,
                 use_rel_bias=True, attn_drop=0.1, temp=1.0, per_channel=True):
        self.delay = LearnableDelayLine(d_in, K, rho, per_channel=per_channel)
        self.sta   = SpikingTemporalAttention(d_in, d, heads, topk, W,
                                              use_rel_bias, attn_drop, temp)
        self.neuron = LIFCell(d, adaptive=True)  # 或接入你现有的 SNN 单元
        self.norm = LayerNorm(d)                 # 可选：Pre/LN
        self.ffn  = MLP(d, d)                    # 可选：残差前馈
    def forward(self, H, S_prev, edge_index, time_idx, adj_mask=None, batch=None):
        H̃ = self.delay(H)                                    # [T,N,d_in]
        M  = self.sta(H̃, S_prev, edge_index, time_idx, adj_mask)  # [T,N,d]
        S, V, aux = self.neuron(M)                            # [T,N], [T,N]
        Y = self.norm(M)                                      # 或对 M+FFN 做残差
        return S, V, Y, {"M": M, **aux}
```

### 整体网络（L 层堆叠）
- 时间维在外层循环或用并行张量化均可。推荐**张量化时间**（形状 `[T,N,·]`）以便 DelayLine 与 STA 使用缓存的 `PE`。  
- 层与层之间传递：`H_{l+1,t} = proj([H_{l,t} || Y_{l,t} || onehot(S_{l,t})])`（可选拼接上一层输出与脉冲 one-hot）。  
- 读出：  
  - 节点分类：`readout_t` 可择 `t=T` 或 `temporal_attention_pool`（轻量单头）  
  - 图级任务：按 batch 聚合（mean/max/attention）

---

## 6. 训练配方（默认值可直接用）

- **优化**：AdamW，`lr=2e-3`，`weight_decay=0.01`；线性 warmup 5% 步数；`grad_clip=1.0`。  
- **surrogate**：`fast_tanh`: \(\sigma'(x)=\beta(1-\tanh^2(\beta x))\)，`β=2.0`（前 10% epoch 用 `β=1.0` 软化）。  
- **正则**：  
  - 脉冲率 L1：\(\lambda_\text{spk}\in[1e-5,5e-5]\) 约束平均发放率；  
  - 注意力熵惩罚（温和）：\(\lambda_\text{ent}=1e-4\)；  
  - 延迟权重 L1/熵：\(\lambda_\text{delay}=1e-4\)。  
- **时间窗/稀疏**：`W=64`，`topk=16`（大图任务可改 `W=32, topk=8`）。  
- **混合精度**：AMP O2；**梯度检查点**：在 STA 内按 `(time block)` 分段。  
- **数据增强（可选）**：时间戳抖动（±1~2 tick），随机时间伸缩（0.9~1.1）。

---

## 7. 掩码与数值稳定性（务必实现）

1) **softmax 掩码**：对被掩蔽位置赋 `-inf`（或非常负的数），再 softmax。  
2) **Top-k**：在 logits 上选 k 大，再将非选中项 logits 置 `-inf`，避免“零后再归一”。  
3) **温度**：`logits /= temp`（`temp∈[0.7,1.4]`），可缓解早期梯度噪声。  
4) **归一**：DelayLine 权重用 `softplus`+`normalize`，数值安全加 `eps=1e-8`。  
5) **空邻居/空窗口**：若 `(i,t)` 无可用 key，返回零向量（或残差直通 `h_{i,t}`）。

---

## 8. 复杂度与内存控制

- **理论**：STA 稀疏实现复杂度 \(O(H\cdot E\cdot W)\)，内存近似同量级；  
- **工程手段**：  
  - 分块时间 `T = sum(T_b)`，逐块缓存 `PE[t−t']`；  
  - 将 `edge_index` 排序（`coalesce`）以提升 `segment_softmax` 命中率；  
  - 对高入度节点可设**邻居 top-k**上限（先按入度采样邻居，再做时序 top-k）。

---

## 9. 相对时间编码实现建议
```python
def rel_time_enc(time_idx, d_pe=8, taus=(4,16), n_freq=3):
    # time_idx: [T], return PE: [T, T, d_pe] for Δt>=0 else zero
    # channels: [exp(-Δt/τ_m), exp(-Δt/τ_s), sin/cos with log-spaced freq, log-bucket onehot]
    ...
```
- 预计算仅对 \(\Delta t \in [0, W]\) 的子矩阵；其余赋零以节省显存。  
- 可选**相对偏置** \(b_{\Delta t}\)（标量表）：长度 `W+1` 的可学习向量。

---

## 10. 模块签名与断言（供代码生成器遵循）

### `LearnableDelayLine`
- `__init__(d_in:int, K:int=5, rho:float=0.85, per_channel:bool=True)`.  
- `forward(H:[T,N,d_in])->[T,N,d_in]`.  
- **断言**：`K>=1`, `0<rho<1`, `H.dim()==3`.

### `SpikingTemporalAttention`
- `__init__(d_in:int, d:int, heads:int=4, topk:int=16, W:int=64, use_rel_bias:bool=True, attn_drop:float=0.1, temp:float=1.0)`.  
- `forward(H_tilde:[T,N,d_in], S:[T,N], edge_index:Long[2,E], time_idx:Long[T], adj_mask:Optional[Bool[N,N]]=None) -> [T,N,d]`.  
- **断言**：`topk>=1`, `W>=1`, `heads*d_head==d`。  
- **稀疏实现关键步骤**：  
  1) 对每条边 `(j->i)` 构造过去窗口 `t'∈[t-W,t]` 的键集合；  
  2) 计算 `q(i,t)` 与 `k(j,t')`，加上相对编码后做点积；  
  3) 在每个 `(i,t)` 的候选集合上做 Top-k，再 masked-softmax；  
  4) 加 `attn_drop`，与 `v(j,t')` 加权求和。

### `LIFCell`
- `__init__(d:int, lambda_mem:float=0.95, tau_theta:float=0.99, gamma:float=0.1, adaptive:bool=True, surrogate:str='fast_tanh', beta:float=2.0)`.  
- `forward(M:[T,N,d])->Tuple[S:[T,N], V:[T,N], aux:Dict]`.

---

## 11. 训练与日志（必须记录的指标）

- 任务指标：Micro/Macro-F1 或 AUC。  
- SNN 指标：平均发放率（全局/分层）、失活率（持续 0 发放）、爆发率（>50% 发放）。  
- STA 指标：平均注意力熵、Top-k 选择比例、相对时间分布（\(\Delta t\) 直方图）。  
- DelayLine 指标：`w_k` 的分布热图；`argmax k` 的频率。  
- 资源指标：每 step 时间、峰值显存。  
- **可视化**：`r_t` 与注意力重心的时间轨迹，`w_k` 热图，`Δt` 权重柱状图。

---

## 12. 消融与开关（实现为 config flags）

- `use_delayline: bool`（False = 仅 STA）  
- `use_sta: bool`（False = 回退到原时间池化）  
- `topk: int in {0->不裁剪, 8, 16, 32}`  
- `W: int`（时间窗）  
- `use_rel_bias: bool`（相对偏置）  
- `per_channel_delay: bool`  
- `surrogate_beta_warmup: bool`（早期软梯度）

---

## 13. 失败模式与守护

- **注意力过密/显存爆**：启用/减小 `topk` 与 `W`；`d_head` 降低；开启分块。  
- **延迟学成平滑**：对 `w_k` 加熵惩罚或“中心惩罚”鼓励峰化；  
- **梯度震荡**：`attn_temperature ↑`、`grad_clip=1.0`、`AdamW β2=0.99`；  
- **空邻居**：返回零向量并走残差；  
- **发放塌陷**：提高 `λ_spk` 下限、软化 surrogate（小 `β`）、降低阈值上调 `γ`。

---

## 14. 参考默认配置（YAML 片段）

```yaml
model:
  d_in: 128
  d: 256
  layers: 3
  heads: 4
  topk: 16
  W: 64
  delayline:
    use: true
    K: 5
    rho: 0.85
    per_channel: true
  sta:
    use_rel_bias: true
    attn_drop: 0.1
    temp: 1.0
  lif:
    lambda_mem: 0.95
    tau_theta: 0.99
    gamma: 0.10
    surrogate: fast_tanh
    beta: 2.0
train:
  lr: 0.002
  weight_decay: 0.01
  grad_clip: 1.0
  amp: true
  seed: 42
regularization:
  l1_spike: 2.0e-5
  attn_entropy: 1.0e-4
  delay_reg: 1.0e-4
```

---

## 15. 最小工作示例（形状检查伪代码）
```python
T, N, d_in, d, H = 64, 1024, 128, 256, 4
H0 = torch.randn(T, N, d_in)
S0 = torch.zeros(T, N)  # 若首层无前序脉冲，可用全 1 门控或上层脉冲
edge_index = ...        # [2,E]
time_idx = torch.arange(T)

layer = SpikeNetXLayer(d_in, d, heads=H, topk=16, W=64, K=5, rho=0.85)
S, V, Y, aux = layer(H0, S0, edge_index, time_idx)
assert S.shape == (T, N) and Y.shape == (T, N, d)
```

---

## 16. 写作要点（供注释/文档使用）
- **创新点**：将**邻居选择（空间）× 时间对齐（时序）**统一到**事件驱动注意力**，并通过 DelayLine 显式建模**传播时延**；  
- **可解释性**：输出 `w_k`、`Δt` 权重与注意力热区；  
- **可扩展性**：STA 与 DelayLine 均为**即插即用**，可替换到任意脉冲/非脉冲时序图骨干。

---